{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.copy_on_write = True\n",
    "custDf = pd.read_csv('customer.tbl', sep='|')\n",
    "dateDf = pd.read_csv('date.tbl', sep='|')\n",
    "partDf = pd.read_csv('part.tbl', sep='|')\n",
    "suppDf = pd.read_csv('supplier.tbl', sep='|')\n",
    "custDf.drop(custDf.columns[[1, 2]], axis=1, inplace=True)\n",
    "suppDf.drop(suppDf.columns[[1, 2]], axis=1, inplace=True)\n",
    "partDf.drop(partDf.columns[1], axis=1, inplace=True)\n",
    "CHUNKSZ = 5000000\n",
    "! mkdir -p s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = custDf[custDf[custDf.columns[1]] == 'UNITED KI1']\n",
    "f2 = custDf[custDf[custDf.columns[1]] == 'UNITED KI5']\n",
    "f3 = suppDf[suppDf[suppDf.columns[1]] == 'UNITED KI1']\n",
    "f4 = suppDf[suppDf[suppDf.columns[1]] == 'UNITED KI5']\n",
    "\n",
    "lsts = [[], [], [], [], []]\n",
    "for i, lioChunk in enumerate(pd.read_csv('lineorder.tbl', sep='|', chunksize = CHUNKSZ)):\n",
    "  use = lioChunk.iloc[:, [2, 4]]\n",
    "  use = use.reset_index()\n",
    "  use.columns = ['LO', 'cusK', 'supK']\n",
    "  joined = use.merge(f1['1'], left_on=use['cusK'], right_on=f1['1'], how='inner')\n",
    "  lsts[0].extend(joined['LO'])\n",
    "  joined = use.merge(f2['1'], left_on=use['cusK'], right_on=f2['1'], how='inner')\n",
    "  lsts[1].extend(joined['LO'])\n",
    "  joined = use.merge(f3['1'], left_on=use['supK'], right_on=f3['1'], how='inner')\n",
    "  lsts[2].extend(joined['LO'])\n",
    "  joined = use.merge(f4['1'], left_on=use['supK'], right_on=f4['1'], how='inner')\n",
    "  lsts[3].extend(joined['LO'])\n",
    "  if i % 5 >= 3: continue\n",
    "  predCol = lioChunk[lioChunk.columns[5]]\n",
    "  lsts[4].extend(lioChunk[(predCol >= 19971200) & (predCol <= 19971300)].index)\n",
    "\n",
    "open('s/s34l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('s/s34l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "open('s/s34l2.txt', 'w').write(','.join(map(str, lsts[2])))\n",
    "open('s/s34l3.txt', 'w').write(','.join(map(str, lsts[3])))\n",
    "open('s/s34l4.txt', 'w').write(','.join(map(str, lsts[4])))\n",
    "len(lsts[0]), len(lsts[1]), len(lsts[2]), len(lsts[3]), len(lsts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsts = [[], [], [], []]\n",
    "f1 = dateDf[(dateDf[dateDf.columns[11]] == 6) &\n",
    "            (dateDf[dateDf.columns[4]] == 1994)]\n",
    "f1 = f1[f1.columns[0]]\n",
    "for i, lioChunk in enumerate(pd.read_csv('lineorder.tbl', sep='|', chunksize = CHUNKSZ)):\n",
    "  use = lioChunk.iloc[:, [5]]\n",
    "  use = use.reset_index()\n",
    "  use.columns = ['LO', 'dateK']\n",
    "  joined = use.merge(f1, left_on=use['dateK'], right_on=f1)\n",
    "  lsts[3].extend(joined['LO'])\n",
    "\n",
    "  predCol = lioChunk[lioChunk.columns[11]]\n",
    "  lsts[0].extend(lioChunk[(predCol >= 4) & (predCol < 7)].index)\n",
    "  predCol = lioChunk[lioChunk.columns[8]]\n",
    "  lsts[1].extend(lioChunk[(predCol >= 26) & (predCol < 36)].index)\n",
    "  # if i % 5 >= 3: continue\n",
    "  predCol = lioChunk[lioChunk.columns[5]]\n",
    "  lsts[2].extend(lioChunk[(predCol >= 19940100) & (predCol <= 19940200)].index)\n",
    "\n",
    "open('s/s12l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('s/s12l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "open('s/s12l2.txt', 'w').write(','.join(map(str, lsts[2])))\n",
    "open('s/s13l2.txt', 'w').write(','.join(map(str, lsts[3])))\n",
    "! ln -sf s12l0.txt s/s13l0.txt\n",
    "! ln -sf s12l1.txt s/s13l1.txt\n",
    "len(lsts[0]), len(lsts[1]), len(lsts[2]), len(lsts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = partDf[partDf[partDf.columns[3]] == 'MFGR#2221']\n",
    "f2 = suppDf[suppDf[suppDf.columns[3]] == 'EUROPE']\n",
    "f1 = f1[f1.columns[0]]\n",
    "f2 = f2[f2.columns[0]]\n",
    "\n",
    "lsts = [[], []]\n",
    "for i, lioChunk in enumerate(pd.read_csv('lineorder.tbl', sep='|', chunksize = CHUNKSZ)):\n",
    "  use = lioChunk.iloc[:, [3, 4]]\n",
    "  use = use.reset_index()\n",
    "  use.columns = ['LO', 'partK', 'suppK']\n",
    "  joined = use.merge(f1, left_on=use['partK'], right_on=f1)\n",
    "  lsts[0].extend(joined['LO'])\n",
    "  joined = use.merge(f2, left_on=use['suppK'], right_on=f2)\n",
    "  lsts[1].extend(joined['LO'])\n",
    "\n",
    "open('s/s23l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('s/s23l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "len(lsts[0]), len(lsts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = partDf[partDf[partDf.columns[1]] == 'MFGR#1']\n",
    "f2 = partDf[partDf[partDf.columns[1]] == 'MFGR#2']\n",
    "f3 = suppDf[suppDf[suppDf.columns[3]] == 'AMERICA']\n",
    "f4 = custDf[custDf[custDf.columns[3]] == 'AMERICA']\n",
    "f1 = f1[f1.columns[0]]\n",
    "f2 = f2[f2.columns[0]]\n",
    "f3 = f3[f3.columns[0]]\n",
    "f4 = f4[f4.columns[0]]\n",
    "\n",
    "lsts = [[], [], [], []]\n",
    "for i, lioChunk in enumerate(pd.read_csv('lineorder.tbl', sep='|', chunksize = CHUNKSZ)):\n",
    "  use = lioChunk.iloc[:, [2, 3, 4]]\n",
    "  use = use.reset_index()\n",
    "  use.columns = ['LO', 'custK', 'partK', 'suppK']\n",
    "  joined = use.merge(f1, left_on=use['partK'], right_on=f1)\n",
    "  lsts[0].extend(joined['LO'])\n",
    "  joined = use.merge(f2, left_on=use['partK'], right_on=f2)\n",
    "  lsts[1].extend(joined['LO'])\n",
    "  joined = use.merge(f3, left_on=use['suppK'], right_on=f3)\n",
    "  lsts[2].extend(joined['LO'])\n",
    "  joined = use.merge(f4, left_on=use['custK'], right_on=f4)\n",
    "  lsts[3].extend(joined['LO'])\n",
    "\n",
    "open('s/s41l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('s/s41l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "open('s/s41l2.txt', 'w').write(','.join(map(str, lsts[2])))\n",
    "open('s/s41l3.txt', 'w').write(','.join(map(str, lsts[3])))\n",
    "len(lsts[0]), len(lsts[1]), len(lsts[2]), len(lsts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.copy_on_write = True\n",
    "orderDf = pd.read_csv('orders.tbl', sep='|', usecols=[0, 1, 4])\n",
    "orderDf.columns = ['orderK', 'custK', 'orderDate']\n",
    "custDf = pd.read_csv('customer.tbl', sep='|', usecols=[0, 6])\n",
    "custDf.columns = ['custK', 'mktSegm']\n",
    "orderDf = orderDf.merge(custDf, left_on=orderDf['custK'], right_on=custDf['custK'])\n",
    "orderDf.drop(['key_0', 'custK_y', 'custK_x'], axis=1, inplace=True)\n",
    "partDf = pd.read_csv('part.tbl', sep='|', usecols=[0, 3, 6])\n",
    "partDf.columns = ['partK', 'brand', 'medBox']\n",
    "CHUNKSZ = 5000000\n",
    "! mkdir -p t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = orderDf[orderDf['mktSegm'] == 'BUILDING']\n",
    "f2 = orderDf[orderDf['orderDate'] >= '1995-04-00']\n",
    "f3 = orderDf[(orderDf['orderDate'] > '1995-03-15') & (orderDf['orderDate'] <= '1995-04-00')]\n",
    "f1 = f1['orderK']\n",
    "f2 = f2['orderK']\n",
    "f3 = f3['orderK']\n",
    "\n",
    "lsts = [[], [], [], [], []]\n",
    "for i, chunk in enumerate(pd.read_csv('lineitem.tbl', sep='|', chunksize=CHUNKSZ, usecols=[0, 10])):\n",
    "  use = chunk.reset_index()\n",
    "  use.columns = ['LO', 'orderK', 'shipDate']\n",
    "  joined = use.merge(f1, left_on=use['orderK'], right_on=f1)\n",
    "  lsts[4].extend(joined['LO'])\n",
    "  joined = use.merge(f2, left_on=use['orderK'], right_on=f2)\n",
    "  lsts[0].extend(joined['LO'])\n",
    "  joined = use.merge(f3, left_on=use['orderK'], right_on=f3)\n",
    "  lsts[1].extend(joined['LO'])\n",
    "\n",
    "  predCol = use['shipDate']\n",
    "  lsts[2].extend(use[predCol < '1995-03-00']['LO'])\n",
    "  lsts[3].extend(use[(predCol > '1995-03-00') & (predCol < '1995-03-15')]['LO'])\n",
    "\n",
    "open('t/t3l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('t/t3l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "open('t/t3l2.txt', 'w').write(','.join(map(str, lsts[2])))\n",
    "open('t/t3l3.txt', 'w').write(','.join(map(str, lsts[3])))\n",
    "open('t/t3l4.txt', 'w').write(','.join(map(str, lsts[4])))\n",
    "len(lsts[0]), len(lsts[1]), len(lsts[2]), len(lsts[3]), len(lsts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsts = [[], [], []]\n",
    "for i, chunk in enumerate(pd.read_csv('lineitem.tbl', sep='|', chunksize=CHUNKSZ, usecols=[4, 6, 10])):\n",
    "  use = chunk.reset_index()\n",
    "  use.columns = ['LO', 'qty', 'disc', 'shipDate']\n",
    "  lsts[0].extend(use[use['qty'] == 24]['LO'])\n",
    "  lsts[1].extend(use[(use['disc'] >= 0.05) & (use['disc'] < 0.07)]['LO'])\n",
    "  lsts[2].extend(use[(use['shipDate'] > '1993') & (use['shipDate'] < '1995')]['LO'])\n",
    "\n",
    "open('t/t6l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('t/t6l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "open('t/t6l2.txt', 'w').write(','.join(map(str, lsts[2])))\n",
    "len(lsts[0]), len(lsts[1]), len(lsts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsts = [[], [], []]\n",
    "for i, chunk in enumerate(pd.read_csv('lineitem.tbl', sep='|', chunksize=CHUNKSZ, usecols=[12, 14])):\n",
    "  use = chunk.reset_index()\n",
    "  use.columns = ['LO', 'recpDate', 'shipMode']\n",
    "  lsts[0].extend(use[use['shipMode'] == 'MAIL']['LO'])\n",
    "  lsts[1].extend(use[use['shipMode'] == 'SHIP']['LO'])\n",
    "  lsts[2].extend(use[(use['recpDate'] > '1994') & (use['recpDate'] < '1994-01-08')]['LO'])\n",
    "open('t/t12l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('t/t12l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "open('t/t12l2.txt', 'w').write(','.join(map(str, lsts[2])))\n",
    "len(lsts[0]), len(lsts[1]), len(lsts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = partDf[partDf['medBox'] == 'MED BOX']['partK']\n",
    "f2 = partDf[partDf['brand'] == 'Brand#23']['partK']\n",
    "lsts = [[], [], []]\n",
    "for i, use in enumerate(pd.read_csv('lineitem.tbl', sep='|', chunksize=CHUNKSZ, usecols=[1, 4])):\n",
    "  use = use.reset_index()\n",
    "  use.columns = ['LO', 'partK', 'qty']\n",
    "  joined = use.merge(f1, left_on=use['partK'], right_on=f1)\n",
    "  lsts[0].extend(joined['LO'])\n",
    "  joined = use.merge(f2, left_on=use['partK'], right_on=f2)\n",
    "  lsts[1].extend(joined['LO'])\n",
    "  lsts[2].extend(use[use['qty'] < 6]['LO'])\n",
    "open('t/t17l0.txt', 'w').write(','.join(map(str, lsts[0])))\n",
    "open('t/t17l1.txt', 'w').write(','.join(map(str, lsts[1])))\n",
    "open('t/t17l2.txt', 'w').write(','.join(map(str, lsts[2])))\n",
    "len(lsts[0]), len(lsts[1]), len(lsts[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
